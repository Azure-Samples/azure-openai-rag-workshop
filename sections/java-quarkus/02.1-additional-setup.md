## Complete the setup

To complete the template setup, please run the following command in a terminal, at the root of the project:

```bash
./scripts/setup-template.sh quarkus
```

### Preparing the environment

<div data-visible="$$proxy$$">

We have deployed an Open AI proxy service for you, so you can use it to work on this workshop locally before deploying anything to Azure.

Create a `.env` file at the root of the project, and add the following content:

```bash
AZURE_OPENAI_API_ENDPOINT=$$proxy$$
QDRANT_URL=http://localhost:6334
```

</div>

<div data-hidden="$$proxy$$">

Now you either have to deploy an Azure Open AI service to use the OpenAI API, or you can use a local LLM with Ollama.

#### Using Azure Open AI

You first need to deploy an Azure Open AI service to use the OpenAI API.

Before moving to the next section, go to the **Azure setup** section (either on the left or using the "hamburger" menu depending of your device) to deploy the necessary resources and create your `.env` file needed.

After you completed the Azure setup, you can come back here to continue the workshop.

</div>

#### (Optional) Using Ollama

If you have a machine with enough resources, you can run this workshop entirely locally without using any cloud resources. To do that, you first have to install [Ollama](https://ollama.com) and then run the following commands to download the models on your machine:

```bash
ollama pull mistral
```

<div class="info" data-title="Note">

> The `mistral` model with download a few gigabytes of data, so it can take some time depending on your internet connection. Using Codespaces will provide you a fast connection.

</div>

<div class="important" data-title="Important">

> Ollama won't work in GitHub Codespaces currently, so it will only work if you are working on the workshop locally.

</div>

<div data-hidden="$$proxy$$">

</div>

Finally, you can start the Ollama server with the following command:

```bash
ollama run mistral
```
